% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/SCADFABS-estimation.R
\name{Lorenz.SCADFABS}
\alias{Lorenz.SCADFABS}
\title{Solves the Penalized Lorenz Regression with SCAD penalty}
\usage{
Lorenz.SCADFABS(
  YX_mat,
  weights = NULL,
  sigma = 1/sqrt(nrow(YX_mat)),
  eps,
  a = 3.7,
  iter = 10^4,
  lambda = "Shi",
  lambda.min = 1e-07,
  gamma = 0.05
)
}
\arguments{
\item{YX_mat}{a matrix with the first column corresponding to the response vector, the remaining ones being the explanatory variables.}

\item{weights}{vector of sample weights. By default, each observation is given the same weight.}

\item{sigma}{value of the parameter of the sigmoid function, determining the smoothness of the approximation of the indicator function. Default value is 1/sqrt(n), where n is the number of observations.}

\item{eps}{step size in the FABS algorithm.}

\item{a}{parameter of the SCAD penalty. Default value is 3.7.}

\item{iter}{maximum number of iterations. Default value is 10^4.}

\item{lambda}{this parameter relates to the regularization parameter. Several options are available.
\describe{
    \item{\code{grid}}{If lambda="grid", lambda is defined on a grid, equidistant in the logarithmic scale.}
    \item{\code{Shi}}{If lambda="Shi", lambda, is defined within the algorithm, as in Shi et al (2018).}
    \item{\code{supplied}}{If the user wants to supply the lambda vector himself}
}}

\item{lambda.min}{lower bound of the penalty parameter. Only used if lambda="Shi".}

\item{gamma}{value of the Lagrange multiplier in the loss function}

\item{w.adaptive}{vector of size equal to the number of covariates where each entry indicates the weight in the adaptive Lasso. By default, each covariate is given the same weight (Lasso).}
}
\value{
A list with several components:
\describe{
   \item{\code{iter}}{number of iterations attained by the algorithm.}
   \item{\code{direction}}{vector providing the direction (-1 = backward step, 1 = forward step) for each iteration.}
   \item{\code{lambda}}{value of the regularization parameter for each iteration.}
   \item{\code{sigma}}{value of the smoothing parameter.}
   \item{\code{theta}}{matrix where column i provides the estimated parameter vector for iteration i.}
   \item{\code{LR2}}{the Lorenz-\eqn{R^2} of the regression.}
   \item{\code{Gi.expl}}{the estimated explained Gini coefficient.}
}
}
\description{
\code{Lorenz.SCADFABS} solves the penalized Lorenz regression with SCAD penalty on a grid of lambda values.
For each value of lambda, the function returns estimates for the vector of parameters and for the estimated explained Gini coefficient, as well as the Lorenz-\eqn{R^2} of the regression.
}
\details{
The regression is solved using the SCAD-FABS algorithm developed by Heuchenne et al (xxxx) and adapted to our case.
For a comprehensive explanation of the Penalized Lorenz Regression, see Heuchenne et al (xxxx).
In order to ensure identifiability, theta is forced to have a L2-norm equal to one.
}
\section{References}{

Heuchenne, C. and A. Jacquemain (2022). Penalized Lorenz Regression.
Shi, X., Y. Huang, J. Huang, and S. Ma (2018). A Forward and Backward Stagewise Algorithm for Nonconvex Loss Function with Adaptive Lasso, \emph{Computational Staistics & Data Analysis 124}, 235-251.
}

\examples{
data(Data.Incomes)
YX_mat <- Data.Incomes
Lorenz.SCADFABS(YX_mat, eps = 0.005)

}
\seealso{
\code{\link{Lorenz.Reg}}, \code{\link{Lorenz.FABS}}
}
