---
title: "Learning Lorenz regressions with examples"
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 2
bibliography: biblio.bib
link-citation: yes
vignette: >
  %\VignetteIndexEntry{Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
  \usepackage[utf8]{inputenc}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The purpose of this vignette is multiple. First, it serves as a manual explaining how to use the `LorenzRegression` package. Second, it motivates the use of the Lorenz regression methodology in inequality measurement through examples on simulated data. Finally, it has a more pedagogical role of providing a broad range of interesting functions for economists working in the area of inequality measurement. 
```{r setup}
library(LorenzRegression)
library(ggplot2)
```

In a nutshell, the Lorenz regression aims to determine the explanatory power of a set of covariates relative to the inequality of a response. To do so, it estimates a measure of explained inequality called the explained Gini coefficient. [The first section](#motivation) defines the relevant concepts present in the literature and illustrate how they can be computed in practice. It also introduces a [decomposition method of the Gini coefficient](#decompo_methods), which correspond to the main "competitor" to Lorenz regressions. We illustrate that the assumption of a linear model, upon which this method is built, can be quite restrictive. 

[//]: # "Add a section on installation of the package"

# Some words on the data {#data}

Throughout this vignette, we use the `Data.Incomes` dataframe. It consists in a fictitious cross-sectional data-frame covering 7 variables for 200 individuals aged between 25 and 30 years. More information is provided when typing `?Data.Incomes`. 

Let us have a first look at the structure of the data. 
```{r}
str(Data.Incomes)
```

# Motivation in inequality measurement {#motivation}

Facing a cross-sectional dataset such as `Data.Incomes`, a microeconomist could be interested in analyzing the inequality pattern of some variable $Y$. In our case, we are interested in income inequality. 

```{r}
Y <- Data.Incomes$Income
```

## The Lorenz curve and the Gini coefficient

A well known tool to analyze the inequality of $Y$ is the **Lorenz curve**. The Lorenz curve of $Y$ at proportion $p\in[0,1]$ is defined as 
\begin{align*}
LC_Y(p) & = \frac{E[YI\{F_Y(Y)\leq p\}]}{E[Y]}
\end{align*}
where $I\{.\}$ is the indicator function and $F_Y$ is the CDF of $Y$. This traces a curve nested between two extreme scenarios. Perfect equality corresponds to the 45° line while perfect inequality is the lower-triangle situation. In our context, it gives the proportion of income obtained by the $p\times100\%$ poorest individuals. We can compute it with function `Lorenz.curve`. The stored object is a function, which takes as single argument a scalar ranging from $0$ to $1$. The argument `graph` can be set to `TRUE` to produce a graph of the obtained Lorenz curve.    
```{r fig.width=6.5, fig.height=4}
LC <- Lorenz.curve(y = Y, graph = T)
p <- 0.1
LC(p)
```

The **Gini coefficient** $G_Y$ is twice the surface between the line of perfect equality and the Lorenz curve. It ranges from $0$ (perfect equality) to $1$ (perfect inequality). We can obtain it with function `Gini.coef`. 

```{r}
Gini <- Gini.coef(y = Y)
Gini
```

## The concentration curve and the concentration index

The **concentration curve** generalizes the Lorenz curve in the bivariate case. The concentration curve of $Y$ with respect to $X$ at proportion $p\in[0,1]$ is defined as
\begin{align*}
CC_{Y,X}(p) & = \frac{E[YI\{F_X(X)\leq p\}]}{E[Y]}
\end{align*}
where $F_X$ is the CDF of $X$.  It can be obtained with function `Lorenz.curve` by specifying the `x` argument.

```{r fig.width=6.5, fig.height=4}
X <- Data.Incomes$Education
CC <- Lorenz.curve(y = Y, x = X, graph = T)
```

In this scenario, $X$ represents the education of the individual in years. At $p$, the concentration curve gives the proportion of income owned by the $p\times100\%$ least-educated individuals.

The **concentration index** $CI_{Y,X}$ is obtained in the same way as the Gini coefficient. As before, $0$ is obtained when all individuals own the same income (perfect equality). However, as we will explain in the example below, we now face two scenarios of extreme inequality represented by values $-1$ and $1$. We can compute a concentration index by specifying the `x` argument in function `Gini.coef`

```{r}
CI_YX <- Gini.coef(y = Y, x = X)
CI_YX
```

More sophisticated graphs can be obtained with function `Lorenz.graphs`. For more information, see `?Lorenz.graphs`. 

### Example 1

We want to examine the Lorenz curve of income as well as the concentration curves of income with respect to two variables : the weekly number of hours worked (`Work.Hours`) and the number of years spent with the current employer (`Seniority`).

```{r fig.width=6.5, fig.height=4}
Data.Ex1 <- Data.Incomes
Lorenz.graphs(formula = Income ~ Work.Hours + Seniority, data = Data.Ex1)
```

#### Why is the green concentration curve above the 45° line ? {#why_above}

The concentration curve of income with respect to seniority is above the 45° line. Intuitively, this means that we observe some income inequality when we rank individuals on the basis of seniority: people who spent less years with their current employer tend to have a higher income. The resulting concentration index is negative, as is shown below.

```{r}
Gini.coef(y = Data.Ex1$Income, x = Data.Ex1$Seniority)
```

The value $-1$ would be attained if the individual with the smallest seniority gathered all the income in the distribution. The fact that the concentration curve is concave simply translates the fact that income is negatively related to senority in the labor market. This happens because we restricted the sample to young individuals, for which the level of experience should matter less than the level of education; and the higher level of education, the lower number of years already spent in the labor force.

If we multiply `Seniority` by $-1$ we obtain a more usual convex concentration curve.

```{r fig.width=6.5, fig.height=4}
Data.Ex1$Seniority.Minus <- - Data.Ex1$Seniority
Lorenz.graphs(formula = Income ~ Work.Hours + Seniority.Minus, data = Data.Ex1)
```

#### Both concentration curves are above the Lorenz curve

As we can observe, both concentration curves are above the Lorenz curve of income. In general terms, it can be proved that $|CI_{Y,X}| < G_Y$. This property helps us build the following interpretation: the concentration index of $Y$ with respect to $X$ is the inequality of $Y$ that one can reproduce if one ranks individuals in terms of $X$ (and not in terms of $Y$). A concentration index of $0$ means that none of the inequality of $Y$ can be attributed to $X$. On the contrary, a value of either $1$ or $-1$ means that all the inequality of $Y$ can be attributed to $X$.

## Decomposition methods {#decompo_methods}

The concentration curve and the concentration index provide a first impression of how a variable $X$ drives the inequality of another variable $Y$. However, we would like to extend this bivariate analysis to a proper multivariate framework.

It can be proved that the Gini coefficient and the concentration index can be expressed as
\begin{align*}
G_{Y} & = \frac{2Cov[Y,F_Y(Y)]}{E[Y]} \\
CI_{Y,X} & = \frac{2Cov[Y,F_X(X)]}{E[Y]}
\end{align*}

Many methods use this property to decompose the Gini coefficient or the concentration index. Indeed, if we assume $Y = X^\intercal\beta + \epsilon$, we can decompose the Gini coefficient as
\begin{align*}
G_Y & = \sum_k w_kCI_{X_k,Y} + GCI_{\epsilon,Y}
\end{align*}
where
\begin{align*}
w_k & = \frac{\beta_k E[X_k]}{E[Y]} \\
GCI_{\epsilon,Y} & = \frac{2Cov[\epsilon,F_Y(Y)]}{E[Y]}
\end{align*}
and analyze the contribution of each of the $X$'s. Such methods are used in differents areas. For example, [@LermanIncomeInequalityEffects1985] decompose the Gini coefficient of income in the contributions of various income sources. In health economics, [@Wagstaffdecomposingcauseshealth2003] present a decomposition of the concentration index of an health outcome.

Beyond their differences, it is important to highlight that all these methods share the same assumption of a linear relationship between $Y$ and the $X$'s. [Example 2](#Ex2) illustrates this point.

### Example 2 {#Ex2}

We wish to decompose the Gini coefficient of income in the contributions of three variables : `Work.Hours`, `Age` and `Health.Level`, where this last variable takes high (low) values when the individual perceives his health to be good (bad). Recall that the Gini coefficient of income is of `r round(Gini,3)`. We can undertake the decomposition as presented above.


```{r}
Data.Ex2 <- Data.Incomes
Fit.Ex2 <- lm(Income ~ Work.Hours + Age + Health.level, data = Data.Ex2)

E_Y <- mean(Data.Ex2$Income)
E_X <- c(mean(Data.Ex2$Work.Hours),mean(Data.Ex2$Age),mean(Data.Ex2$Health.level))
beta <- coef(Fit.Ex2)[-1]
w_k <- beta*E_X/E_Y

CI_k <- c()
CI_k[1] <- Gini.coef(y = Data.Ex2$Work.Hours, x = Data.Ex2$Income)
CI_k[2] <- Gini.coef(y = Data.Ex2$Age, x = Data.Ex2$Income)
CI_k[3] <- Gini.coef(y = Data.Ex2$Health.level, x = Data.Ex2$Income)

GCI_eps <- Gini - sum(w_k*CI_k)

Contributions <- data.frame(w_k = w_k, CI = CI_k, Total = w_k*CI_k)
round(Contributions,4)
```

As we can observe, the age doesn't seem to contribute much to income inequality. However, the error term $GCI_{\epsilon,Y}=$ `r round(GCI_eps,3)` is quite large; `r round(GCI_eps/Gini*100,3)`% of the observed inequality is left unexplained. What's more, it is crucial that the linear model holds. In that respect, it is interesting to analyze a graph of the residuals $\hat{\epsilon}_i = Y_i - \hat{Y}_i$ in function of the predicted values $\hat{Y}_i$.

```{r fig.width=6.5, fig.height=4}
library(ggplot2)
library(NonpModelCheck)
Resid.fit.Ex2 <- localpoly.reg(X=Fit.Ex2$fitted.values,Y=Fit.Ex2$residuals,points=Fit.Ex2$fitted.values,
                               degree.pol=1,kernel.type="gaussian")$predicted
Data.lm <- data.frame(residuals = Fit.Ex2$residuals, fitted.values = Fit.Ex2$fitted.values, residuals.fitted = Resid.fit.Ex2)
ggplot(Data.lm)+
  geom_point(aes(x = fitted.values, y = residuals), size = 0.5)+
  geom_line(aes(x = fitted.values, y = residuals.fitted), col = "red")+
  geom_hline(yintercept = 0, color = "blue")+
  labs(x = "Fitted values", y = "Residuals", title = "Model fit of the linear model")
```

The graph above shows a clear relationship between the residuals of the linear model and its predicted values. The linear model seems to fall short in explaining the relationship between incomes and the chosen covariates. While this explains partly the size of the error term $GCI_{\epsilon,Y}$, it is important to understand that the issue doesn't stop here. As the linear assumption is ruled out, the decomposition procedure in itself becomes invalid. 

With this example in mind, we wish to have a methodology which

* stresses the main drivers of the observed inequality of $Y$;
* rests on more flexible assumptions than a linear relationship between $Y$ and the $X$'s.

# The Lorenz regression methodology

Usual methods exploit a linearity assumption ($Y = X^\intercal\beta + \epsilon$) to decompose the inequality measure of $Y$ in the contributions of each of the $X$'s. We propose to tackle the question differently. We introduce a measure of explained inequality, the **explained Gini coefficient**. It is defined as the maximum value that can be attained by the concentration index of $Y$ with respect to a linear combination of the $X$'s. Our purpose is to estimate this quantity.

## Explained Gini coefficient

We focus on the concentration curve of $Y$ with respect to $X^\intercal\theta$, where $\theta$ is a weight vector such that $\sum_k|\theta_k|=1$. Intuitively, it is the income inequality that we observe if individuals are ranked in terms of the index $X^\intercal\theta$. Using the data from [Example 2](#Ex2) with an arbitrary weight vector, we have the following.

```{r fig.width=6.5, fig.height=4}
theta <- c(0.5,0.3,0.2)
Data.Ex2$Index <-
  theta[1]*Data.Ex2$Work.Hours +
  theta[2]*Data.Ex2$Age +
  theta[3]*Data.Ex2$Health.level
Lorenz.graphs(formula = Income ~ Index, data = Data.Ex2)
Gini.coef(y = Data.Ex2$Income, x = Data.Ex2$Index)
```

Of course, the resulting concentration index heavily depends on the choice of $\theta$, as shown in the example below.

```{r}
theta2 <- c(0.2,0.5,0.3)
Data.Ex2$Index2 <-
  theta2[1]*Data.Ex2$Work.Hours +
  theta2[2]*Data.Ex2$Age +
  theta2[3]*Data.Ex2$Health.level
Gini.coef(y = Data.Ex2$Income, x = Data.Ex2$Index2)
Gini.coef(y = Data.Ex2$Income, x = Data.Ex2$Index)
```

The **Lorenz regression** chooses the weight vector $\theta$ which maximizes the concentration index of $Y$ with respect to $X^\intercal\theta$. The resulting Lorenz curve is called the **explained Lorenz curve** and the resulting concentration index is called the **explained Gini coefficient** ($\text{Gi}_{Y,X}$). Finally, the **Lorenz-R²** ($\text{LR}^2$) is defined as the ratio between the explained and the observed Gini coefficients. We can easily prove that this measure lies between $0$ and $1$. It will be close to $0$ when the $X$'s don't drive the inequality of $Y$; it will be close to $1$ when much of the inequality of $Y$ can be explained by the $X$'s.

## The estimation procedure

A consistent estimator for the explained Gini coefficient is obtained by solving the following optimization programme.
\begin{align*}
\hat{\text{Gi}}_{Y,X} & = \max_\theta \frac{2}{n^2}\sum_{i=1}^n \frac{Y_i}{\overline{Y}}R_n(X_i^\intercal\theta) - \frac{n+1}{n} \qquad \text{s.t. } \sum_k |\theta_k| = 1 \text{,}
\end{align*}
where $R_n(X^\intercal\theta)$ is the rank vector associated to $X^\intercal\theta$, i.e. $R_n(X_i^\intercal\theta)=1$ if observation $i$ is the smallest in terms of $X^\intercal\theta$ and $R_n(X_j^\intercal\theta)=n$ if observation $j$ is the largest in terms of $X^\intercal\theta$. Since the rank operator is discrete, the objective function is nonconvex and non differentiable. Function `Lorenz.Reg` estimates the optimal weight vector using a genetic algorithm. It also returns the estimated explained Gini coefficient and the Lorenz-R². A more precise description of the function is given in [Example 3](#Ex3)

```{r}
LR.Ex2 <- Lorenz.Reg(formula = Income ~ Work.Hours + Age + Health.level, data = Data.Ex2)
LR.Ex2$theta
LR.Ex2$LorenzR2
LR.Ex2$expl.Gini
```

Note that all the signs of the obtained weights are positive. A higher income is attached to older individuals, working longer, and with a better health leve. The explained Gini coefficient is of `r round(LR.Ex2$expl.Gini,3)`. Finally, these three variables explain `r round(LR.Ex2$LorenzR2*100,3)`$\%$ of the observed income inequality.

### Example 3 {#Ex3}

Compared to [Example 2](#Ex2), we enlarge our analysis to include the remaining explanatory variables: `Sex`, `Education` and `Seniority`. Let us have a look at the `Lorenz.Reg` function
```{r}
?Lorenz.Reg
```

If the argument `standardize` is set to `TRUE`, the numeric explanatory variables are standardized before solving the genetic algorithm. The obtained vector of weights is then transformed to match the original scale of the covariates. 

```{r}
LR.Ex3 <- Lorenz.Reg(formula = Income ~ . , data = Data.Incomes)

LR.Ex3$theta
LR.Ex3$LorenzR2
```

Note that the Lorenz-R² is now of `r round(LR.Ex3$LorenzR2*100,3)`$\%$.

## Inference on the weight vector

### The single-index model

Contrary to the decomposition methods presented above, the described procedure does not rest on a model relationship between $Y$ and the $X$'s. We simply attach a weight to each covariate such that the explanatory power of $X$ in reproducing the inequality of $Y$ is fully exploited (in the sense of computing the explained Gini coefficient).

This being said, it will be interesting to make a connection with the single-index model, used in the econometrics litterature. Following [@HorowitzSingleIndexModels2009a], we define the single-index model as
\begin{align*}
E[Y|X=x] &  = H(x^\intercal\theta_0)
\end{align*}
where $||\theta_0||=1$. Here, we furthermore assume that $H$ is increasing. [^1]

[^1]: The assumption of monotonicity of $H$ is needed for the consistency of the MRE. It is justified in our context by our objective. We want to find a synthetic index of income such that large values of this index are associated to large incomes.

Interestingly, the weight vector obtained via Lorenz regression is a special case of the monotone rank estimator (MRE) for $\theta_0$ proposed by [@CavanaghRankestimatorsmonotonic1998] and for which asymptotic properties have been derived. Hence, **we can use these results to build tests or confidence intervals for the weights of a Lorenz regression, or more importantly, for the explained Gini coefficient**.

### Estimation of H

Estimating $H$ will be interesting in different respects. For example, we will need this to undertake the bootstrap exercise developed in the [next section](#bootstrap).

Given that $\theta$ is obtained by the Lorenz regression procedure, we have a vector of indices $T_i = X_i^\intercal\theta$. We can estimate $H$ using the non-parametric estimator proposed by [@ChernozhukovImprovingpointinterval2009], which incoroporates the assumption of monotonicity. This estimator can be derived using function `Rearrangement.estimation`.

We illustrate this procedure with the data from [Example 2](#Ex2). More information is provided in the help of the function, see `?Rearrangement.estimation`.

```{r fig.width=6.5, fig.height=4}
Index.Ex2 <- as.vector(LR.Ex2$theta%*%t(Data.Ex2[,c("Work.Hours","Age","Health.level")]))
Y.Ex2 <- Data.Ex2$Income
H.Ex2 <- Rearrangement.estimation(Y.Ex2, Index.Ex2)$H
```

Note that the single-index is more flexible than the classical linear model. This can be seen in the plot of the residuals with respect to the fitted values. 

```{r fig.width=6.5, fig.height=4}
Resid.LR.Ex2 <- Y.Ex2 - H.Ex2
Resid.fit.LR.Ex2 <- localpoly.reg(X=H.Ex2,Y=Resid.LR.Ex2,points=H.Ex2,
                               degree.pol=1,kernel.type="gaussian")$predicted
Data.Lorenz <- data.frame(residuals = Resid.LR.Ex2, fitted.values = H.Ex2, residuals.fitted = Resid.fit.LR.Ex2)
ggplot(Data.Lorenz)+
  aes(x = fitted.values, y = residuals)+
  labs(x = "Fitted values", y = "Residuals", title = "Model fit of the single-index model")+
  geom_point(aes(x = fitted.values, y = residuals),size=0.5)+
  geom_line(aes(x = fitted.values, y = residuals.fitted), col = "red")+
  geom_hline(yintercept = 0, color = "blue")
```

Contrary to the clear trend outlined previously, we do not observe any clear structure in the residuals. 

### Bootstrap confidence intervals and tests {#bootstrap}

While the estimates of the explained Gini coefficient and of the weight vector provide a first piece of information, a more precise inference can prove to be useful. Function `Lorenz.boot` allows us to perform the following inference exercises using bootstrap: 

* build confidence intervals for the explained Gini coefficient;
* build confidence intervals for the weight attached to each covariate;
* provide individual p-values indicating the significance of each covariate;
* conduct a test of joint significance of several covariates.

The following example illustrates the two first points with the data used in [Example 3](#Ex3). Note that the following argument are used. 

* `B = 20`. The number of bootstrap resamples. In practice, we advise to use no less than $200$ bootstrap resamples. Here, the value of $20$ is used to speed computation time. 
* `Lorenz.est = LR.Ex3` provides the estimated vector $\hat{\theta}$ to avoid loss of computation time. Default value is `NULL` and $\hat{\theta}$ is then computed internally. 
* `testorCI = "CI"` indicates that we want to compute confidence intervals. Other possible values are `test` if we want to undertake a test of joint significance and `both` if we want both confidence intervals and to undertake a test of joint significance. 
* `which.CI = c("Basic","Perc","Param")` indicates the type of bootstrap we want to use. `Param` uses the limiting distribution of $\hat{\theta}$ and only bootstraps the variance of $\hat{\theta}$. The other two methods bootstrap the distribution of the estimator $\hat{\theta}$; `Basic` denotes basic bootstrap and `Perc` indicates percentile bootstrap. If `Param` is chosen, we can also directly compute p-values for the significance of each $\theta_k$. These p-values are returned. 
* `parallel = FALSE`. If set to `TRUE`, parallel computing is used in order so speed up the bootstrapping procedure. 

```{r}
set.seed(1)
Boot.each <- Lorenz.boot(formula = Income ~ ., 
data = Data.Incomes, 
B = 20, 
Lorenz.est = LR.Ex3,
testorCI = "CI",
which.CI = c("Basic","Perc","Param"),
parallel = FALSE)

Boot.each

```

The three methods seem to provide similar confidence intervals. As an illustration, note that the p-value associated to `Age` is of `r round(Boot.each$Param.pvalues[3]*100,3)`%. Hence, at a significance level of $5\%$, we would conclude that, given the other covariates, the age does not contribute to income inequality. 
  
## References
